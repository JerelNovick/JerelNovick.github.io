{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in cvs and assigned it to a dataframe\n",
    "df_all = pd.read_csv('./data/all_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Created X and y\n",
    "X = df_all['full_text']\n",
    "y = df_all['subreddit']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Calculated the count and percentage breakdown of subreddits (y).\n",
    "# 0 = jokes\n",
    "# 1 = dad jokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The dataframe is unbalance. There will be a bias to predicting 0 (jokes).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Created a modified stopword list to include certain contractions.\n",
    "new_stopwords = set(stopwords.words('english')) - {\"can't\",\"don't\",\"i'm\",\"let's\",\"you're\",\"i'll\",\n",
    "                                                   \"we'll\",\"wouldn't\",\"who's\",\"should've\",\"could've\",\"isn't\",\"hasn't\",\n",
    "                                                  \"aren't\",\"haven't\",\"has'nt\",\"o'clock\",\"ma'am\",\"mustn't\",\n",
    "                                                  \"how'd\",\"how's\",\"didn't\",\"you'll\",\"she'll\",\"he'll\",\"it'll\",\"they'll\",\n",
    "                                                  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_tokenizer(s):\n",
    "    return s.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer(analyzer = \"word\",\n",
    "                             tokenizer = my_tokenizer,\n",
    "                             preprocessor = None,\n",
    "                             stop_words = new_stopwords, max_features=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform X and create array of words\n",
    "X_train_features = cvec.fit_transform(X_train)\n",
    "X_test_features = cvec.transform(X_test)\n",
    "X_train_array  = X_train_features.toarray()\n",
    "X_test_array = X_test_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiated logistic regression model\n",
    "lr = LogisticRegression(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit to the model to training data\n",
    "lr.fit(X_train_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scored against training data\n",
    "lr.score(X_train_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scored against test data\n",
    "lr.score(X_test_features, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Created a predictions variable\n",
    "pred = lr.predict(X_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ran an accuracy score\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Based partially on code idea via Google\n",
    "cm = confusion_matrix(y_test,pred) #Assigned matrix\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel() #Broke out elements of matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('True Negatives: ', tn)\n",
    "print('False Positives: ', fp)\n",
    "print('False Negatives: ', fn)\n",
    "print('True Positives: ', tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print out classification metrics\n",
    "accuracy = round((((tp+tn)/(tp+fn+tn+fp))*100),2)\n",
    "misclassification = round(((1 - (accuracy/100))*100),2)\n",
    "sensitivy = round(((tp/(tp+fn))*100),2)\n",
    "specificity = round(((tn/(tn+fp))*100),2)\n",
    "precision = round(((tp/(tp+fp))*100),2)\n",
    "print('Accuracy Rate:',accuracy,'%')\n",
    "print('Misclassification Rate:',misclassification,'%')\n",
    "print('Sensitivy Rate:',sensitivy,'%')\n",
    "print('Specificity Rate:',specificity,'%')\n",
    "print('Precision Rate:',precision,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Created a dataframe of the confusion matrix and printed it out.\n",
    "cm_df = pd.DataFrame(cm, columns=['pred negative', 'pred positive'], index=['actual negative', 'actual positive'])\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The model has bias to predicting negatives (0) which are Jokes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomized the predictions.  The original data is split 60/40 so I reversed it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_preds = [np.random.choice([0,1],1, p=[0.4,0.6])[0] for _ in range(len(pred))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, simulated_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The random model is less accuarate than the model I created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrix\n",
    "cm = confusion_matrix(y_test,simulated_preds)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, simulated_preds).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('True Negatives: ', tn)\n",
    "print('False Positives: ', fp)\n",
    "print('False Negatives: ', fn)\n",
    "print('True Positives: ', tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification metrics\n",
    "accuracy = round((((tp+tn)/(tp+fn+tn+fp))*100),2)\n",
    "misclassification = round(((1 - (accuracy/100))*100),2)\n",
    "sensitivy = round(((tp/(tp+fn))*100),2)\n",
    "specificity = round(((tn/(tn+fp))*100),2)\n",
    "precision = round(((tp/(tp+fp))*100),2)\n",
    "print('Accuracy Rate:',accuracy,'%')\n",
    "print('Misclassification Rate:',misclassification,'%')\n",
    "print('Sensitivy Rate:',sensitivy,'%')\n",
    "print('Specificity Rate:',specificity,'%')\n",
    "print('Precision Rate:',precision,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_df = pd.DataFrame(cm, columns=['pred negative', 'pred positive'], index=['actual negative', 'actual positive'])\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Established pipeline\n",
    "pipe = Pipeline([('cvec', CountVectorizer()),\n",
    "                 ('lr', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Created model with many parameters\n",
    "pipe_params = {\n",
    "    'cvec__max_features': [3000, 4000, 5000,6000,7000,8000],\n",
    "    'cvec__min_df': [1,2,3,4,5],\n",
    "    'cvec__max_df': [0.5,0.6,0.7],\n",
    "    'cvec__ngram_range': [(1,1), (1,2),(1,3)]\n",
    "}\n",
    "gs = GridSearchCV(pipe, param_grid=pipe_params, cv=5, n_jobs=5,verbose=2)\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_score_)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scored model against traing data\n",
    "gs.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scored model against test data\n",
    "gs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = gs.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrix\n",
    "cm = confusion_matrix(y_test,pred)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('True Negatives: ', tn)\n",
    "print('False Positives: ', fp)\n",
    "print('False Negatives: ', fn)\n",
    "print('True Positives: ', tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification metrics\n",
    "accuracy = round((((tp+tn)/(tp+fn+tn+fp))*100),2)\n",
    "misclassification = round(((1 - (accuracy/100))*100),2)\n",
    "sensitivy = round(((tp/(tp+fn))*100),2)\n",
    "specificity = round(((tn/(tn+fp))*100),2)\n",
    "precision = round(((tp/(tp+fp))*100),2)\n",
    "print('Accuracy Rate:',accuracy,'%')\n",
    "print('Misclassification Rate:',misclassification,'%')\n",
    "print('Sensitivy Rate:',sensitivy,'%')\n",
    "print('Specificity Rate:',specificity,'%')\n",
    "print('Precision Rate:',precision,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_df = pd.DataFrame(cm, columns=['pred negative', 'pred positive'], index=['actual negative', 'actual positive'])\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Based on code idea I googled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Created model\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('cvec', cvec),\n",
    "    ('mnb', mnb)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train, y_train)\n",
    "print('train score:', pipe.score(X_train, y_train))\n",
    "print('test score:', pipe.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = mnb.predict(X_test_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrix\n",
    "cm = confusion_matrix(y_test,pred)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('True Negatives: ', tn)\n",
    "print('False Positives: ', fp)\n",
    "print('False Negatives: ', fn)\n",
    "print('True Positives: ', tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification metrics\n",
    "accuracy = round((((tp+tn)/(tp+fn+tn+fp))*100),2)\n",
    "misclassification = round(((1 - (accuracy/100))*100),2)\n",
    "sensitivy = round(((tp/(tp+fn))*100),2)\n",
    "specificity = round(((tn/(tn+fp))*100),2)\n",
    "precision = round(((tp/(tp+fp))*100),2)\n",
    "print('Accuracy Rate:',accuracy,'%')\n",
    "print('Misclassification Rate:',misclassification,'%')\n",
    "print('Sensitivy Rate:',sensitivy,'%')\n",
    "print('Specificity Rate:',specificity,'%')\n",
    "print('Precision Rate:',precision,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_df = pd.DataFrame(cm, columns=['pred negative', 'pred positive'], index=['actual negative', 'actual positive'])\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chart of the top 10 ten words in each subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Based on code idea from Google\n",
    "dad_words = pd.DataFrame(X_train_features.todense(), \n",
    "                          columns=cvec.get_feature_names()).reindex(y_train[y_train == 1].index)\n",
    "top_dad_word = dad_words.mean().sort_values(ascending=False).iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_dad_word = top_dad_word.sort_values(ascending=True)\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.barh(top_dad_word.index, top_dad_word.values,height=.5,color=['red', 'green', 'navy', 'orange'])\n",
    "plt.rc('xtick', labelsize=20)\n",
    "plt.rc('ytick', labelsize=20)\n",
    "plt.title('Dad Word Frequency', size=18);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joke_words = pd.DataFrame(X_train_features.todense(), \n",
    "                          columns=cvec.get_feature_names()).reindex(y_train[y_train == 0].index)\n",
    "top_joke_word = joke_words.mean().sort_values(ascending=False).iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_joke_word = top_joke_word.sort_values(ascending=True)\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.barh(top_joke_word.index, top_joke_word.values,height=.5,color=['red', 'green', 'navy', 'orange'])\n",
    "plt.rc('xtick', labelsize=20)\n",
    "plt.rc('ytick', labelsize=20)\n",
    "plt.title('Joke Word Frequency', size=18);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# There is signifigant overlap of the top ten words between the subrebbits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
